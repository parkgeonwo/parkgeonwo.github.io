---
layout: post
title: deep-learning-theory-post6
description: >
  
sitemap: false
hide_last_modified: true
categories:
  - deep-learning
  - theory
---

## 내가 다시 보려고 만든 "밑바닥 부터 시작하는 딥러닝" 정리 4

책 "밑바닥부터 시작하는 딥러닝 1권" 내용 중 다시 보려고 만든 자료입니다. (챕터 6)

### chapter 6

- 이번 장에서 다룰 주제는 가중치 매개변수의 최적값을 탐색하는 최적화 방법, 가중치 매개변수 초깃값, 하이퍼파라미터 설정 방법 등 신경망 학습에서 중요한 주제이다. 오버피팅의 대응책인 가중치 감소와 드롭아웃등의 정규화 방법도 설명하고 배치 정규화도 짧게 알아본다.

### chapter 6.1 매개변수 갱신

- 신경망 학습의 목적은 손실 함수의 값을 가능한 한 낮추는 매개변수를 찾는 것이었다. 이는 곧 매개변수의 최적값을 찾는 문제이며, 이러한 문제를 푸는 것을 <strong>최적화</strong>라고 한다.

### chapter 6.1.2 확률적 경사 하강법(SGD)

- SGD의 수식
   ![100x100](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FFs4Y3%2FbtqFt6ndQjT%2F4QCJLDwHBIgAdwYbeoac01%2Fimg.png)

### chapter 6.1.3 SGD의 단점

- SGD에 의한 최적화 갱신 경로 : 최솟값인 (0,0)까지 지그재그로 이동하니 비효율적이다.
   ![100x100](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fc87nGX%2FbtqFtKdIfso%2Fapokj19hld4v1EKY4rydD1%2Fimg.png)

- SGD의 단점은 비등방성 함수(방향에 따라 성질, 즉 여기에서는 기울기가 달라지는 함수)에서는 탐색 경로가 비효율적이라는 것이다.

### chapter 6.1.4 모멘텀

- 모멘텀은 운동량을 뜻하는 단어다.
   ![150x100](https://mblogthumb-phinf.pstatic.net/MjAxODAyMDFfMTMz/MDAxNTE3NDEyNDkzOTY3.xc165v0N5R1xe5ajJO9g5z2sB-CQy3RwT1dfLIg9B3Eg.l09OGIx4-QRW6LjjiaSz7I30NQUfXVzHkEqgFZn0W1og.PNG.worb1605/image.png?type=w800)

   - W는 갱신할 가중치 매개변수, dL/dW 는 W에 대한 손실 함수의 기울기, η는 학습률이다. v 라는 변수가 새로 나오는데, 이는 물리에서 말하는 속도에 해당한다. 위의 식은 기울기 방향으로 힘을 받아 물체가 가속된다는 물리 법칙을 나타낸다.
   - αv항은 물체가 아무런 힘을 받지 않을 때 서서히 하강시키는 역할을 한다. 물리에서의 지면 마찰이나 공기 저항에 해당한다.

   - 모멘텀에 의한 최적화 갱신 경로
   ![200x200](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FpWahY%2FbtqJ1KMBFiI%2FSJ8P6njpgqqy7FXH2qKKjk%2Fimg.png)
   - 지그재그 정도가 덜한 것을 알 수 있다. 이는 x축의 힘은 아주 작지만 방향은 변하지 않아서 한 방향으로 일정하게 가속하기 때문이다. 거꾸로 y축의 힘은 크지만 위아래로 번갈아 받아서 상충하여 y축 방향의 속도는 안정적이지 않다. 전체적으로는 SGD 보다 x축 방향으로 빠르게 다가가 지그재그 움직임이 줄어든다.

### chapter 6.1.5 AdaGrad

- 학습률을 정하는 효과적 기술로 <strong>학습률 감소</strong>이 있다. AdaGrad는 개별 매개변수에 적응적으로 학습률을 조정하면서 학습을 진행한다. AdaGrad 갱신 방법의 수식은 다음과 같다.
   - ![200x100](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FH8MY9%2FbtqJ0owhSUL%2Fj7keQZ0PVQfy7RMS9F9tyK%2Fimg.png)
  
   - 매개변수를 갱신할 때 1/((h)^1/2)을 곱해 학습률을 조정한다. 매개변수의 원소 중에서 많이 움직인(크게 갱신된) 원소는 학습률이 낮아진다는 뜻인데, 다시 말해 학습률 감소가 매개변수의 원소마다 다르게 적용됨을 뜻한다.

   - AdaGrad에 의한 최적화 갱신 경로
   ![200x200](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcHAg5e%2FbtqJWTcMctz%2F6foaiv3ZaaD7mrJ8ZvaRe0%2Fimg.png)

- RMSProp
   - AdaGrad는 과거의 기울기를 제곱하여 계속 더해간다. 그래서 학습을 진행할수록 갱신 강도가 약해진다. 실제로 무한히 계속 학습한다면 어느 순간 갱신량이 0이 되어 전혀 갱신되지 않게된다. 이 문제를 개선한 기법으로 RMSProp라는 방법이 있다. 과거의 모든 기울기를 균일하게 더하는 것이 아니라, 먼 과거의 기울기는 서서히 잊고 새로운 기울기 정보를 크게 반영한다. 이를 지수이동평균이라 하여, 과거 기울기의 반영 규모를 기하급수적으로 감소시킨다.

### chapter 6.1.6 Adam

- 모멘텀과 AdaGrad 두 기법을 융한한 기법 : Adam
- 두 기법의 이점을 조합하여 매개변수 공간을 효율적으로 탐색해준다. 또한, 하이퍼파라미터의 '편향 보정'이 진행된다는 점도 Adam의 특징이다.
- Adam에 의한 최적화 갱신 경로
   ![200x200](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fdt8EvM%2FbtqJWrm9RLv%2FkqUDwalmGKKWswlZ8waVoK%2Fimg.png)

### chapter 6.2 가중치의 초깃값

- 신경망 학습에서 특히 중요한 것이 가중치의 초깃값이다.

### chapter 6.2.1 초깃값을 0으로 하면 ?

- 오버피팅을 억제해 범용 성능을 높이는 테크닉인 <strong>가중치 감소</strong> 기법을 소개한다. 가중치 감소는 간단히 말하자면 가중치 매개변수의 값이 작아지도록 학습하는 방법이다. 가중치 값을 작게 하여 오버피팅이 일어나지 않게 하는 것이다.

- 그러나 가중치의 초깃값을 0으로 하면 안된다. 왜 그럴까? (정확히는 가중치를 균일한 값으로 설정해서는 안된다.) 그 이유는 바로 오차역전파법에서 모든 가중치의 값이 똑같이 갱신되기 때문이다. 가중치들은 초깃값에서 시작하고 갱신을 거쳐도 여전히 같은 값을 유지하기 때문이다. 이는 가중치를 여러개 갖는 의미를 사라지게 한다. 이 '가중치가 고르게 되어버리는 상황'을 막으려면 (정확히는 가중치의 대칭적인 구조를 무너뜨리려면) 초깃값을 무작위로 설정해야 한다.

### chapter 6.2.2 은닉층의 활성화값 분포

- 은닉층의 활성화값(활성화 함수의 출력 데이터)의 분포를 관찰하면 중요한 정보를 얻을 수 있습니다.
- 가중치를 표준편차가 1인 정규분포로 초기화할 때의 각 층의 활성화값 분포
   ![300x100](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbXwKX9%2FbtqJV5kvD2x%2FFsKOhrjF1hgpVWwqgqs3b1%2Fimg.png)

- 각 층의 활성화값들이 0과 1에 치우쳐 분포되어있다. 그래서 데이터가 0과 1에 치우쳐 분포하게 되면 역전파의 기울기 값이 점점 작아지다가 사라집니다. 이것은 <strong>기울기 소실</strong>이라 알려진 문제입니다.

- 가중치를 표준편차가 0.01인 정규분포로 초기화할 때의 각 층의 활성화값 분포
   ![300x100](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F3Cc0d%2FbtqJ1Lkwt5r%2Fugja9FTEK9LXageJ8p5wF1%2Fimg.png)

- 이번에는 0.5 부근에 집중되었다. 이 상황에서는 다수의 뉴런이 거의 같은 값을 출력하고 있으니 뉴런을 여러 개 둔 의미가 없어진다. 그래서 활성화 값을이 치우치면 <strong>표현력을 제한</strong>한다는 관점에서 문제가 된다.

- 각 층의 활성화값은 적당히 골고루 분포되어야 한다. 층과 층 사이에 적당하게 다양한 데이터가 흐르게 해야 신경망 학습이 효율적으로 이뤄지기 때문이다. 반대로 치우친 데이터가 흐르면 기울기 소실이나 표현력 제한 문제에 빠져서 학습이 잘 이뤄지지 않는 경우가 생긴다.

- 이를 해결하기 위해서 권장하는 가중치인 <strong>Xavier 초깃값</strong>을 사용한다. 앞 계층의 노드가 n개라면 표준편차가 (1/n)^(1/2)인 분포를 사용하면 된다.

- 가중치의 초깃값으로 'Xavier 초깃값'을 이용할 때의 각 층의 활성화값 분포
   ![300x100](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FwHCjZ%2FbtqJ6dURrVt%2FlfJyeGZBqO8kL5h22AllE0%2Fimg.png)

- 위의 그림은 오른쪽으로 갈수록 약간씩 일그러진다. 이 일그러짐은 sigmoid 대신 tanh 함수(쌍곡선 함수)를 이용하면 개선된다. 활성화 함수용으로는 원점에서 대칭인 함수가 바람직하다고 알려져있다.

- Relu를 사용할 때의 가중치 초깃값은 <strong>He 초깃값</strong>을 사용한다. 앞 계층의 노드가 n개일 때, 표준편차가 (2/n)^(1/2)인 정규분포를 사용한다.
   ![400x400](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FU9XNE%2FbtqJZmsnc1t%2FYpvIHLUctK9ghmdHbERTJ0%2Fimg.png)

- 결과를 보면 std = 0.01일 때의 각 층의 활성화값들은 아주 작은 값이다. 신경망에 아주 작은 데이터가 흐르는 것은 역전파 때 가중치의 기울기 역시 작아진다는 뜻이다. 이는 실제로 학습이 거의 이뤄지지 않을 것이다. Xavier 초깃값은 치우침이 조금씩 커진다. 이는 '기울기 소실' 문제를 일으킨다. He 초깃값은 모든 층에서 균일하게 분포되었다. 층이 깊어져도 분포가 균일하게 유지되게이 역전파 때도 적절한 값이 나올 것으로 기대할 수 있다.

### chapter 6.3 배치 정규화

- 가중치의 초깃값을 적절히 설정하면 활성화값 분포가 적당히 퍼지면서 학습이 잘된다. 그러나 각 층이 활성화를 적당히 퍼뜨리도록 '강제'해보면 어떨까? 이런 아이디어에서 <strong>배치 정규화</strong>가 출발했다.

### chapter 6.3.1 배치 정규화 알고리즘

- 배치 정규화가 주목받는 이유
   - 학습을 빨리 진행할 수 있다.(학습 속도 개선)
   - 초깃값에 크게 의존하지 않는다.(골치아픈 초깃값 선택 장애 안녕)
   - 오버피팅을 억제한다(드롭아웃 등의 필요성 감소)

- 배치 정규화의 기본 아이디어는 각 층에서의 활성화 값이 적당히 분포되도록 조정하는 것이다.
- 배치 정규화를 사용한 신경망의 예
   ![300x100](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile29.uf.tistory.com%2Fimage%2F994586445BBE000E15CC3D)

- 배치 정규화는 그 이름과 같이 학습 시 미니배치를 단위로 정규화한다. 구체적으로는 데이터 분포가 평균이 0, 분산이 1이 되도록 정규화 한다.

### chapter 6.4.1 오버피팅

- 오버피팅은 주로 다음 두 경우에 일어난다.
   - 매개변수가 많고 표현력이 높은 모델
   - 훈련 데이터가 적음

### chapter 6.4.2 가중치 감소

- 오버피팅 억제용으로 예로부터 많이 이용해온 방법이 <strong>가중치 감소</strong>이다. 이는 학습 과정에서 큰 가중치에 대해서는 그에 상응하는 큰 페널티를 부과하여 오버피팅을 억제하는 방법이다.

### chapter 6.4.3 드롭아웃

- 신경망 모델이 복잡해지면 가중치 감소만으로는 대응하기 어려워진다. 이럴 때는 흔히 <strong>드롭 아웃</strong>이라는 기법을 이용한다.

- 드롭아웃은 뉴런을 임의로 삭제하면서 학습하는 방법이다. 훈련 때 은닉층의 뉴런을 무작위로 골라 삭제한다. 훈련 때는 데이터를 흘릴 때마다 삭제할 뉴런을 무작위로 선택하고, 시험 때는 모든 뉴런에 신호를 전달한다. 단, 시험 때는 각 뉴런의 출력에 훈련 때 삭제 안 한 비율을 곱하여 출력한다.

- 앙상블 학습
   - 앙상블 학습은 개별적으로 학습시킨 여러 모델의 출력을 평균 내어 추론하는 방식이다. 앙상블 학습은 드롭아웃과 밀접하다.
   - 드롭아웃이 학습 때 뉴런을 무작위로 삭제하는 행위를 매번 다른 모델을 학습시키는 것으로 해석할 수 있기 때문이다. 그리고 추론 때는 뉴런의 출력에 삭제한 비율(이를테면 0.5등)을 곱함으로써 앙상블 학습에서 여러 모델의 평균을 내는 것과 같은 효과를 얻는 것이다. 즉, 드롭아웃은 앙상블 학습과 같은 효과를 (대략) 하나의 네트워크로 구현했다고 생각할 수 있다.

### chapter 적절한 하이퍼파라미터 값 찾기

- 하이퍼파라미너의 성능을 평가할 때는 시험 데이터를 사용하면 안된다. 왜? 그것은 시험 데이터를 사용하여 조절하면 하이퍼파라미터 값이 시험 데이터에 오버피팅되기 때문이다. 시험 데이터에만 적합하도록 조정되어 버린다.

- 그래서 하이퍼 파라미터를 조정할 떄는 하이퍼파라미터 전용 확인 데이터가 필요하다. 하이퍼파라미터 조정용 데이터를 일반적으로 <strong>검증 데이터</strong>라고 부른다.
   - 훈련 데이터 : 매개변수 학습
   - 검증 데이터 : 하이퍼파라미터 성능 평가
   - 시험 데이터 : 신경망의 범용 성능 평가

- 하이퍼파라미터 최적화
   - 하이퍼파라미터 최적화의 핵심은 '최적 값'이 존재하는 범위를 조금씩 줄여간다는 것이다. 범위를 조금씩 줄이면 대략적인 범위를 설정하고 그 범위에서 무작위로 하이퍼파라미터 값을 골라낸(샘플링) 후, 그 값으로 정확도 평가한다. 정확도를 잘 살피면서 이 작업을 여러 번 반복하여 '최적 값'의 범위를 좁혀가는 것이다.

   - 0단계 : 하이퍼파라미터 값의 범위를 설정
   - 1단계 : 설정된 범위에서 하이퍼파라미터의 값을 무작위로 추출
   - 2단계 : 1단계에서 샘플링한 하이퍼파라미터 값을 사용하여 학습하고, 검증 데이터로 정확도를 평가(에폭은 작게)
   - 3단계 : 1,2 단계를 특정 횟수(100회 등) 반복하며, 그 정확도의 결과를 보고 하이퍼파라미터 범위를 좁힌다.

   - 이상을 반복하여 하이퍼파라미터의 범위를 좁히고 어느 정도 좁아지면 그 압축한 범위에서 값을 하나 고른다.













