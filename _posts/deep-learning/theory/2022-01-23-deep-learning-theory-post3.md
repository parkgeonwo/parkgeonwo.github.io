---
layout: post
title: deep-learning-theory-post2
description: >
  
sitemap: false
hide_last_modified: true
categories:
  - deep-learning
  - theory
---

## 내가 다시 보려고 만든 "밑바닥 부터 시작하는 딥러닝" 정리

책 "밑바닥부터 시작하는 딥러닝 1권" 내용 중 다시 보려고 만든 자료입니다. (챕터 2, 3)

### chapter 2.1 : 퍼셉트론

- 입력 신호가 뉴런에 보내질 때는 각각 고유한 <strong>가중치</strong>가 곱해진다. (x1w1, x2w2)
- 뉴런에서 보내온 신호의 총합이 정해진 한계를 넘어설 때만 1을 출력하고, 그 한계를 <strong>임계값</strong>이라고 한다.
- 가중치는 전류에서 말하는 <strong>저항</strong>과 비슷하다. 서로 작용하는 방향은 반대이지만 신호의 흐름을 통제한다는 점에서 같은 기능을 한다.

### chapter 2.2 : 단순한 논리 회로

- <strong>학습</strong>이란 적절한 매개변수 값을 정하는 작업이며, 사람은 퍼셉트론의 구조(모델)을 고민하고 컴퓨터에 학습할 데이터를 주는 일을 합니다.

### chapter 2.3 : 퍼셉트론 구현하기

- w1, w2는 각 입력 신호가 결과에 주는 영향력(중요도)을 조절하는 매개변수이고, 편향은 뉴런이 얼마나 쉽게 활성화(결과로 1을 출력)하느냐를 조정하는 매개변수이다.
- 편향의 값은 ㄴ런이 얼마나 쉽게 활성화 되는지를 결정한다.

### chapter 2.5 : 다층 퍼셉트론이 충돌한다면

- 퍼셉트론의 아름다움은 '층을 쌓아' <strong>다층 퍼셉트론</strong>을 만들 수 있다는 데 있다.

### chapter 3. 신경망

- chpater2에서의 좋은 소식은 퍼셉트론으로 복잡한 함수도 표현할 수 있다는 것.
- 나쁜 소식은 가중치를 설정하는 작업(원하는 결과를 출력하도록 가중치 값을 적절히 정하는 작업)은 여전히 사람이 수동으로 해야 한다는 것.
- but 신경망은 나쁜소식을 해결해준다. -> 가중치 매개변수의 적절한 값을 데이터로부터 자동으로 학습하는 능력이 신경망의 중요한 성질이다.

### chapter 3.1 : 퍼셉트론에서 신경망으로

- 입력 신호의 총합을 출력 신호로 변환하는 함수를 일반적으로 <strong>활성화 함수</strong>라고 합니다.
- '활성화'라는 이름이 말해주듯 활성화 함수 입력 신호의 총합이 활성화를 일으키는지를 정하는 역할을 합니다.
- 가중치 신호를 조합한 결과가 a라는 노드가 되고, 활성화 함수 h()를 통과하여 y라는 노드로 변환하는 과정이 나타난다.

### chapter 3.2 : 활성화 함수

- 0과 1로 결과가 출력되는 함수는 임계값을 경계로 출력이 바뀌는데, 이런 함수를 계단 함수라 합니다.
- 시그모이드 함수 : 앞 장에서 본 퍼셉트론과 앞으로 볼 신경망의 주된 차이는 이 활성화 함수이다.
   - 1 / ( 1+exp(-x) )

- 시그모이드 함수와 계단 함수 비교
   - 가장 먼저 느껴지는 차이는 '매끄러움'이다. 시그모이드는 입력에 따라 출력이 연속적 변화 / 계단 함수는 0을 경계로 출력이 갑자기 변화
   - 공통점은 둘다 입력이 작을 때는 0에 가깝고, 입력이 커지면 1에 가깝다. 또한, 입력이 아무리 작거나 크더라도 0과 1사이이다.

- 비선형 함수
   - 계단 함수와 시그모이드는 둘다 비선형 함수이다.
   - 신경망에선 활성화함수를 비선형 함수를 이용해야 한다.
   - why ? 선형 함수의 문제는 층을 아무리 깊게해도 '은닉층이 없는 네트워크'로도 똑같은 기능을 할 수 있기 때문이다. ( ex. h(x) = cx )

- relu 함수
   - h(x) = x (x>0) / 0 (x<=0)

### chapter 3.4 : 3층 신경망 구현하기

- 출력층의 활성화 함수는 풀고자 하는 문제의 성질에 맞게 정해야 한다.
- 회귀에는 항등 함수 / 2클래스 분류는 시그모이드 / 다중 클래스 분류에는 소프트 맥수 함수를 사용

### chapter 3.5 : 출력층 설계하기

- 소프트맥스 함수

![200x150](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F7o3ns%2FbtqvQDIyhq4%2FFYgVfbO6NaJrkc7y11f440%2Fimg.png)

n은 출력층의 뉴런 수, yk는 그중 k번째 출력임을 뜻함, 분자는 입력 신호 ak의 지수함수 , 분모는 모든 입력 신호의 지수 함수의 합

- 소프트맥스 함수 구현 시 문제점
   - 컴퓨터로 계산할 때는 오버플로 문제가 발생
   - 컴퓨터는 수를 4바이트나 8바이트와 같이 크기가 유한한 데이터로 다룬다. 다시 말해 표현할 수 있는 범위가 한정되어 너무 큰 값은 표현할 수 없다는 문제가 발생하고 이것을 오버플로라 하며, 컴퓨터로 수치를 계산할 때 주의 해야한다.

![200x300](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fbq9WqB%2FbtqvR3GAKu3%2FSKFEuT1Md6BUwKUOEeNbfk%2Fimg.png)

오버플로 개선한 소프트맥스 식

결과 : 소프트맥스의 지수 함수를 계산할 때 어떤 정수를 더해도 (혹은 빼도) 결과는 바뀌지 않는다.

- 소프트 맥수 함수의 특징
   - 출력은 0에서 1.0 사이의 실수이며, 총 합은 1이다. 따라서 함수의 출력을 '확률'로 해석할 수 있다.
   - 주의점으로, 소프트맥수 함수를 적용해도 각 원소의 대소 관계는 변하지 않는다. --> 신경망으로 분류 시, 출력층의 소프트맥스 함수를 생략해도 된다.
     - 기계학습의 문제 풀이는 <strong>학습</strong>과 <strong>추론</strong>의 두 단계를 거친다.
     - 추론 단계에서는 출력층의 소프트맥스 함수를 생략하는게 일반적이고
     - 학습 단계에서는 출력층에서 소프트맥스 함수를 사용한다.

### chapter 3.6 : 손글씨 숫자 인식

- 파이썬의 pickle : 프로그램 실행 중에 특정 객체를 파일로 저장하는 기능 , 저장해둔 pickle 파일을 로드하면 실행 당시의 객체를 즉시 복원할 수 있다.

- 배치 처리
   - 배치 처리는 컴퓨터로 계산할 떄 큰 이점을 준다.
   - 첫 번째 이유 : 수치 계산 라이브러리 대부분이 큰 배열을 효율적으로 처리할 수 있도록 고도로 최적화 되어있기 때문
   - 두 번째 이유 : 커다란 신경망에서는 데이터 전송이 병목으로 작용하는 경우가 자주 있는데, 배치 처리를 함으로써 버스에 주는 부하를 줄인다.
   - 컴퓨터는 큰 배열을 한꺼번에 계산하는 것이 분할된 작은 배열을 여러 번 계산하는 것보다 빠르다.




